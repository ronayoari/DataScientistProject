{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Recurrent Neural Network - Final Project**\n",
    "---\n",
    "***Authors:*** Ronit Yoari & Orel Swisa  \n",
    "***Date:*** August 8, 2016  \n",
    "***[Dataset](http://www.gutenberg.org/cache/epub/8001/pg8001.txt)***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Choosing the data collection sequences\n",
    "---\n",
    "## - Collection of data that we have chosen for this project is:  \n",
    "We were looking to investigate a subject that interests us both, and we decided on a ***biblical writing style***.  \n",
    "The first book in the bible - **Genesis** is the one that we choose to explore.  \n",
    "  \n",
    "## - Link to dataset:  \n",
    "[Gensis](http://www.gutenberg.org/cache/epub/8001/pg8001.txt)\n",
    "  \n",
    "## - Data Example - Reading first data records  \n",
    "1. In the beginning God created the heaven and the earth.  \n",
    "  \n",
    "2. And the earth was without form, and void; and darkness was  \n",
    "upon the face of the deep. And the Spirit of God moved upon  \n",
    "the face of the waters.  \n",
    "  \n",
    "3. And God said, Let there be light: and there was light.  \n",
    "  \n",
    "4. And God saw the light, that it was good: and God divided the  \n",
    "light from the darkness.  \n",
    "  \n",
    "5. And God called the light Day, and the darkness he called  \n",
    "Night. And the evening and the morning were the first day.  \n",
    "  \n",
    "6. And God said, Let there be a firmament in the midst of the  \n",
    "waters, and let it divide the waters from the waters.  \n",
    "  \n",
    "7. And God made the firmament, and divided the waters which were  \n",
    "under the firmament from the waters which were above the  \n",
    "firmament: and it was so.  \n",
    "  \n",
    "8. And God called the firmament Heaven. And the evening and the  \n",
    "morning were the second day.  \n",
    "  \n",
    "9. And God said, Let the waters under the heaven be gathered  \n",
    "together unto one place, and let the dry land appear: and it  \n",
    "was so.  \n",
    "  \n",
    "10. And God called the dry land Earth; and the gathering together  \n",
    "of the waters called he Seas: and God saw that it was good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data Description\n",
    "---\n",
    "## Description of data collection chosen:  \n",
    "The Book of Genesis is the first book of the Hebrew Bible (the Tanakh) and the Christian Old Testament.  \n",
    "The basic narrative expresses the central theme: God creates the world (along with creating the first man and woman) and appoints man as his regent, but man proves disobedient and God destroys his world through the Flood. The new post-Flood world is equally corrupt, but God does not destroy it, instead calling one man, Abraham, to be the seed of its salvation. At God's command Abraham descends from his home into the land of Canaan, given to him by God, where he dwells as a sojourner, as does his son Isaac and his grandson Jacob. Jacob's name is changed to Israel, and through the agency of his son Joseph, the children of Israel descend into Egypt, 70 people in all with their households, and God promises them a future of greatness. Genesis ends with Israel in Egypt, ready for the coming of Moses and the Exodus. The narrative is punctuated by a series of covenants with God, successively narrowing in scope from all mankind (the covenant with Noah) to a special relationship with one people alone (Abraham and his descendants through Isaac and Jacob).  \n",
    "## The main challenges of working with this data were:  \n",
    "### The main difficulties that we faced with them during the work were:  \n",
    "- Finding a source that contains the text.  \n",
    "- Pre-processing is required (done in Notepad ++) to adjust the text to the form that we could work with it.  \n",
    "## Why the study interesting and where he can contribute?  \n",
    "- Research on this data is interesting because it deals with an ancient text written thousands of years ago, and we wonder what features it has and whether we can although the ancient writing style to create a meaningful new information logically.  \n",
    "- This research can contribute to understanding the biblical writing style - whether it is similar to current writing style? What has changed? And to answer many other questions that arise in this topic.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Data Preprocessing\n",
    "---\n",
    "## Stage 1: TOKENIZE TEXT  \n",
    "We have raw text, but we want to make predictions on a per-word basis.  \n",
    "This means we must tokenize our comments into sentences, and sentences into words.   \n",
    "At first, we want to choose the dictionary that we'll work with him.  \n",
    "We choose the collection of the most common words in the text.   \n",
    "Tokens:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCABULARY_SIZE = int(os.environ.get('VOCABULARY_SIZE', '2500'))\n",
    "vocabulary_size = _VOCABULARY_SIZE\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data, get raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Reading file...\"\n",
    "path = '../Genesis.txt'\n",
    "f = open(path)\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text file into sentences\n",
    "\n",
    "Comments into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: PREPEND SPECIAL START AND END TOKENS\n",
    "We also want to learn which words tend start and end a sentence.  \n",
    "To do this we prepend a special ***SENTENCE_START*** token, and append a special ***SENTENCE_END*** token to each sentence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**  \n",
    "Parsed 1467 sentences.  \n",
    "## Stage 3: REMOVE INFREQUENT WORDS  \n",
    "Most words in our text will only appear one or two times.  \n",
    "It’s a good idea to remove these infrequent words. Having a huge vocabulary will make our model slow to train, and because we don’t have a lot of contextual examples for such words we wouldn’t be able to learn how to use them correctly anyway.  \n",
    "\n",
    "Tokenize the sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "print 'after tokenized_sentences'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**  \n",
    "Found 2633 unique words tokens. \n",
    " \n",
    "Get the most common words and build index_to_word and word_to_index vectors\n",
    "limit our vocabulary to the vocabulary_size most common words (which was set to 2500)\n",
    "The word UNKNOWN_TOKEN will become part of our vocabulary and we will predict it just like any other word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace all words not included in our vocabulary by **UNKNOWN_TOKEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**  \n",
    "Using vocabulary size 2500.  \n",
    "The least frequent word in our vocabulary is 'friends' and appeared 1 times.\n",
    "\n",
    "\n",
    "Replace all words not in our vocabulary with the **unknown token** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: TRAINING DATA MATRICES  \n",
    "The input to our Recurrent Neural Networks are vectors, not strings. So we create a mapping between words and indices.  \n",
    "**Create the training data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. The Code in python with RNN algorithm used to generate the data recovery model\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The training function:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=1, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "            # ADDED! Saving model oarameters\n",
    "            save_model_parameters_theano(\"../data/rnn-theano-%d-%d-%s.npz\" % (model.hidden_dim, model.word_dim, time), model)\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            print i\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Data generating code sequences according to the model being studied\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code that generate a new sentence according to the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERATING TEXT\n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Building and operating model, using it for creating information\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import RNNTheano\n",
    "\n",
    "model = RNNTheano(vocabulary_size, hidden_dim=_HIDDEN_DIM)\n",
    "tStart = time.time()\n",
    "model.sgd_step(X_train[10], y_train[10], _LEARNING_RATE)\n",
    "tEnd = time.time()\n",
    "print \"SGD Step time: %f milliseconds\" % ((tEnd - tStart) * 1000.)\n",
    "\n",
    "if _MODEL_FILE != None:\n",
    "    load_model_parameters_theano(_MODEL_FILE, model)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, nepoch=_NEPOCH, learning_rate=_LEARNING_RATE)\n",
    "\n",
    "print 'end traing'\n",
    "\n",
    "##GENERATING TEXT\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while operator.gt(len(sent),senten_min_length):\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output:    \n",
    "- and god said unto him to you shall be his servant  \n",
    "- of the sons of jacob's firstborn  \n",
    "- these are the days of heaven were covered  \n",
    "- god called the lord said i will give it came to drink  \n",
    "- and the morning were the fifth day  \n",
    "- and they beginning of every one speech  \n",
    "- then he begat salah lived after thee  \n",
    "- and i pray thee a little food  \n",
    "- and every living creature that the lord be clear ourselves  \n",
    "- and take to keep me for ever  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Evaluation of quality information reconstructed by comparing the sequences synthesized\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function check similarity between the generated sentences and the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for preforming similarity between texts.\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tv\n",
    "\n",
    "# Load the texts - The original & The generated\n",
    "gensis = open('../dataset/Genesis.txt','r').read().split('\\r')\n",
    "model_gensis = open('../outputFiles/Model_Genesis.txt','r').read().split('\\n')\n",
    "\n",
    "# Initialize\n",
    "TfV = tv()\n",
    "TfV.fit(gensis)\n",
    "Y = TfV.transform(gensis)\n",
    "\n",
    "# Check for every sentence the similarity\n",
    "similaritySum = 0\n",
    "for sentence in model_gensis:\n",
    "    X = TfV.transform([sentence])\n",
    "    print(sentence)\n",
    "    print(gensis[cs(X, Y).argmax()])\n",
    "    print(' ')\n",
    "    similaritySum = cs(X, Y).max()\n",
    "\n",
    "# Calculate the similarity\n",
    "similarity = similaritySum/len(model_gensis)\n",
    "print('The similarity between the original text - Genesis -  and the model is: ' , similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output:  \n",
    "  \n",
    "**Generated:** and god said unto him to you shall be his servant  \n",
    "**Original:**  And he said, Blessed be the LORD God of Shem; and Canaan shall be his servant.  \n",
    "   \n",
    "**Generated:** of the sons of jacob's firstborn  \n",
    "**Original:**  And these are the names of the children of Israel, which came into Egypt, Jacob and his sons: Reuben, Jacob's firstborn.  \n",
    "   \n",
    "**Generated:** these are the days of heaven were covered  \n",
    "**Original:**  And the waters prevailed exceedingly upon the earth; and all the high hills, that were under the whole heaven, were covered.   \n",
    "   \n",
    "**Generated:** god called the lord said i will give it came to drink  \n",
    "**Original:**  And, behold, the LORD stood above it, and said, I am the LORD God of Abraham thy father, and the God of Isaac: the land whereon thou liest, to thee will I give it, and to thy seed;  \n",
    "   \n",
    "**Generated:** and the morning were the fifth day   \n",
    "**Original:**  And the evening and the morning were the fifth day.  \n",
    "   \n",
    "**Generated:** and they beginning of every one speech  \n",
    "**Original:**  And the whole earth was of one language, and of one speech.  \n",
    "   \n",
    "**Generated:** then he begat salah lived after thee  \n",
    "**Original:**  And Arphaxad begat Salah; and Salah begat Eber.  \n",
    "   \n",
    "**Generated:** and i pray thee a little food  \n",
    "**Original:**  And our father said, Go again, and buy us a little food.  \n",
    "   \n",
    "**Generated:** and every living creature that the lord be clear ourselves  \n",
    "**Original:**  And with every living creature that is with you, of the fowl, of the cattle, and of every beast of the earth with you; from all that go out of the ark, to every beast of the earth.   \n",
    "   \n",
    "**Generated:** and take to keep me for ever  \n",
    "**Original:**  For all the land which thou seest, to thee will I give it, and to thy seed for ever.  \n",
    "   \n",
    "('The similarity between the original text - Genesis -  and the model is: ', 0.057482169031696125)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Analysis of the study results and conclusions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we feel that this project helped us a lot in understanding the work of the Data Scientist.\n",
    "Require that we perform the following steps:\n",
    "- Selecting a data set.\n",
    "- Preprocessing phase.\n",
    "- Analysis of the information through - RNN.\n",
    "- Check similarity - attesting to the quality of the results.\n",
    "\n",
    "It was interesting to examine the biblical text with emphasis on writing style.\n",
    "You can see that the results actually were of a high similarity to original text.\n",
    "\n",
    "Deep learning requires many computing resources - CPU, GPU and a lot of memory.\n",
    "These resources are limiting the depth calculation that we could make,\n",
    "and if we had the necessary resources, we could add extra layers and additional iterations -\n",
    "things that would improve the results even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}